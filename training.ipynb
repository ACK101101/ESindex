{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndexDataset import IndexDataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset_name = \"data/imdb/imdb_top_1000.csv\"\n",
    "# column_name = \"Overview\"\n",
    "dataset_name = \"data/lego/inventory_parts.csv\"\n",
    "column_name = \"part_num\"\n",
    "batch_size = 128\n",
    "\n",
    "df = pd.read_csv(dataset_name)\n",
    "dataset = IndexDataset(df, column_name)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LitIndexer(L.LightningModule):\n",
    "    def __init__(self, mapper, indexer, name):\n",
    "        super().__init__()\n",
    "        self.mapper = mapper\n",
    "        self.indexer = indexer\n",
    "        self.min_loss = float(\"inf\")\n",
    "        self.epoch_losses = []\n",
    "        self.test_losses = []\n",
    "        self.results_df = pd.DataFrame(columns=[\"Name\", \"Test Loss\", \"Percent Narrowed\"])\n",
    "        \n",
    "    def forward(self, strs):\n",
    "        mapped_strs = self.mapper.forward(strs)\n",
    "        pred_idxs = self.indexer(mapped_strs)\n",
    "        return pred_idxs\n",
    "        \n",
    "    def loss(self, pred_idxs, real_idxs):\n",
    "        return F.mse_loss(pred_idxs, real_idxs)\n",
    "    \n",
    "    def percent_narrowed(self, loss):\n",
    "        return math.sqrt(loss) / len(self.mapper.data)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        strs, idxs = batch\n",
    "        mapped_strs = self.mapper.forward(strs)\n",
    "        pred_idxs = self.indexer(mapped_strs)\n",
    "        loss = self.loss(pred_idxs, idxs.to(torch.float32))\n",
    "        self.min_loss = min(self.min_loss, loss)\n",
    "        self.epoch_losses.append(loss)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        avg_epoch_loss = sum(self.epoch_losses) / len(self.epoch_losses)\n",
    "        print(f\"Epoch {self.current_epoch}, Average Epoch Loss: {avg_epoch_loss:.4f}, Percent Narrowed: {self.percent_narrowed(avg_epoch_loss):.4f}\")\n",
    "        self.epoch_losses = []\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        strs, idxs = batch\n",
    "        mapped_strs = self.mappers(strs)\n",
    "        pred_idxs = self.indexer(mapped_strs)\n",
    "        loss = self.loss(pred_idxs, idxs)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        avg_test_loss = sum(self.test_losses) / len(self.test_losses)\n",
    "        print(f\"Average Test Loss: {avg_test_loss:.4f}, Percent Narrowed: {self.percent_narrowed(avg_test_loss):.4f}\")\n",
    "        self.test_losses = []\n",
    "        self.results_df = self.results_df.append({\"Experiment\": self.name, \n",
    "                                                  \"Test Loss\": avg_test_loss, \n",
    "                                                  \"Percent Narrowed\": self.percent_narrowed(avg_test_loss)}, \n",
    "                                                 ignore_index=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BaselineHash import BaselineHash\n",
    "from models.BaselineEmbed import BaselineHash\n",
    "from models.LinearModel import LinearRegressionModel\n",
    "import mmh3\n",
    "\n",
    "experiments = {\n",
    "            \"__name\": {\"dataset\": None, \"mapper\": None, \"indexer\": None}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "for name, experiment_dict in experiments.items():\n",
    "    dataset, mapper, indexer = experiment_dict[\"dataset\"], experiment_dict[\"mapper\"], experiment_dict[\"indexer\"]\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = LitIndexer(mapper, indexer, name)\n",
    "    \n",
    "    tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/')\n",
    "    trainer = L.Trainer(accelerator=\"cpu\", logger=tb_logger)\n",
    "    trainer.fit(model, train_dataloaders=dataloader)\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"models/{name}.pth\")\n",
    "    \n",
    "    trainer.test(f\"models/{name}.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BaselineHash import BaselineHash\n",
    "from models.LinearModel import LinearRegressionModel\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import mmh3\n",
    "\n",
    "hash = mmh3.hash    # 32-bits\n",
    "mapper = BaselineHash(dataset, hash)\n",
    "indexer = LinearRegressionModel(1)\n",
    "\n",
    "dataset = mapper.data\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = LitIndexer(mapper, indexer)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/')\n",
    "trainer = L.Trainer(accelerator=\"cpu\", logger=tb_logger)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/basehash_lego.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitIndexer(\n",
       "  (indexer): LinearRegressionModel(\n",
       "    (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitIndexer(mapper, indexer)\n",
    "model.load_state_dict(torch.load(\"models/basehash_lego.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "for strs, idxs in test_dataloader:\n",
    "    pred_idx = round(model(strs).item())\n",
    "    print(pred_idx, idxs.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/alexanderkumar/miniconda3/envs/graphs/lib/python3.9/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alexanderkumar/miniconda3/envs/graphs/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\n",
      "  | Name    | Type                  | Params\n",
      "--------------------------------------------------\n",
      "0 | mapper  | BaselineEmbed         | 40    \n",
      "1 | indexer | LinearRegressionModel | 16    \n",
      "--------------------------------------------------\n",
      "56        Trainable params\n",
      "0         Non-trainable params\n",
      "56        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/alexanderkumar/miniconda3/envs/graphs/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36360e6f1e6d46178e1515b51af89206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/5hbdnj_57xv861zqcz43bs6w0000gn/T/ipykernel_8355/3299457662.py:21: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(pred_idxs, real_idxs)\n",
      "/var/folders/9c/5hbdnj_57xv861zqcz43bs6w0000gn/T/ipykernel_8355/3299457662.py:21: UserWarning: Using a target size (torch.Size([91])) that is different to the input size (torch.Size([91, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(pred_idxs, real_idxs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Epoch Loss: 177892656.0000, Percent Narrowed: 0.5766\n",
      "Epoch 1, Average Epoch Loss: 174817744.0000, Percent Narrowed: 0.5716\n",
      "Epoch 2, Average Epoch Loss: 168488800.0000, Percent Narrowed: 0.5612\n",
      "Epoch 3, Average Epoch Loss: 159550592.0000, Percent Narrowed: 0.5461\n",
      "Epoch 4, Average Epoch Loss: 148804304.0000, Percent Narrowed: 0.5274\n",
      "Epoch 5, Average Epoch Loss: 136966352.0000, Percent Narrowed: 0.5060\n",
      "Epoch 6, Average Epoch Loss: 124619840.0000, Percent Narrowed: 0.4826\n",
      "Epoch 7, Average Epoch Loss: 112519488.0000, Percent Narrowed: 0.4586\n",
      "Epoch 8, Average Epoch Loss: 101114984.0000, Percent Narrowed: 0.4347\n",
      "Epoch 9, Average Epoch Loss: 90910808.0000, Percent Narrowed: 0.4122\n",
      "Epoch 10, Average Epoch Loss: 82065712.0000, Percent Narrowed: 0.3916\n",
      "Epoch 11, Average Epoch Loss: 74734544.0000, Percent Narrowed: 0.3737\n",
      "Epoch 12, Average Epoch Loss: 69012056.0000, Percent Narrowed: 0.3591\n",
      "Epoch 13, Average Epoch Loss: 64633840.0000, Percent Narrowed: 0.3476\n",
      "Epoch 14, Average Epoch Loss: 61424152.0000, Percent Narrowed: 0.3388\n",
      "Epoch 15, Average Epoch Loss: 59139436.0000, Percent Narrowed: 0.3325\n",
      "Epoch 16, Average Epoch Loss: 57347180.0000, Percent Narrowed: 0.3274\n",
      "Epoch 17, Average Epoch Loss: 56101640.0000, Percent Narrowed: 0.3238\n",
      "Epoch 18, Average Epoch Loss: 54931048.0000, Percent Narrowed: 0.3204\n",
      "Epoch 19, Average Epoch Loss: 53905380.0000, Percent Narrowed: 0.3174\n",
      "Epoch 20, Average Epoch Loss: 53016756.0000, Percent Narrowed: 0.3148\n",
      "Epoch 21, Average Epoch Loss: 52158788.0000, Percent Narrowed: 0.3122\n",
      "Epoch 22, Average Epoch Loss: 51352116.0000, Percent Narrowed: 0.3098\n",
      "Epoch 23, Average Epoch Loss: 50642372.0000, Percent Narrowed: 0.3077\n",
      "Epoch 24, Average Epoch Loss: 49914276.0000, Percent Narrowed: 0.3054\n",
      "Epoch 25, Average Epoch Loss: 49307212.0000, Percent Narrowed: 0.3036\n",
      "Epoch 26, Average Epoch Loss: 48767520.0000, Percent Narrowed: 0.3019\n",
      "Epoch 27, Average Epoch Loss: 48263840.0000, Percent Narrowed: 0.3003\n",
      "Epoch 28, Average Epoch Loss: 47852132.0000, Percent Narrowed: 0.2991\n",
      "Epoch 29, Average Epoch Loss: 47462920.0000, Percent Narrowed: 0.2978\n",
      "Epoch 30, Average Epoch Loss: 47141048.0000, Percent Narrowed: 0.2968\n",
      "Epoch 31, Average Epoch Loss: 46868960.0000, Percent Narrowed: 0.2960\n",
      "Epoch 32, Average Epoch Loss: 46636728.0000, Percent Narrowed: 0.2952\n",
      "Epoch 33, Average Epoch Loss: 46454936.0000, Percent Narrowed: 0.2947\n",
      "Epoch 34, Average Epoch Loss: 46272220.0000, Percent Narrowed: 0.2941\n",
      "Epoch 35, Average Epoch Loss: 46152152.0000, Percent Narrowed: 0.2937\n",
      "Epoch 36, Average Epoch Loss: 46001344.0000, Percent Narrowed: 0.2932\n",
      "Epoch 37, Average Epoch Loss: 45909496.0000, Percent Narrowed: 0.2929\n",
      "Epoch 38, Average Epoch Loss: 45816440.0000, Percent Narrowed: 0.2926\n",
      "Epoch 39, Average Epoch Loss: 45744108.0000, Percent Narrowed: 0.2924\n",
      "Epoch 40, Average Epoch Loss: 45667528.0000, Percent Narrowed: 0.2922\n",
      "Epoch 41, Average Epoch Loss: 45615208.0000, Percent Narrowed: 0.2920\n",
      "Epoch 42, Average Epoch Loss: 45558624.0000, Percent Narrowed: 0.2918\n",
      "Epoch 43, Average Epoch Loss: 45548144.0000, Percent Narrowed: 0.2918\n",
      "Epoch 44, Average Epoch Loss: 45493084.0000, Percent Narrowed: 0.2916\n",
      "Epoch 45, Average Epoch Loss: 45422428.0000, Percent Narrowed: 0.2914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderkumar/miniconda3/envs/graphs/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from models.BaselineEmbed import BaselineEmbed\n",
    "from models.LinearModel import LinearRegressionModel\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "token_len = 1\n",
    "embed_size = 1\n",
    "mapper = BaselineEmbed(dataset, token_len, embed_size)\n",
    "indexer = LinearRegressionModel(mapper.max_len * embed_size)\n",
    "\n",
    "model = LitIndexer(mapper, indexer)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/')\n",
    "trainer = L.Trainer(accelerator=\"cpu\", logger=tb_logger)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/baseembed_lego.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitIndexer(\n",
       "  (mapper): BaselineEmbed(\n",
       "    (embed): Embedding(40, 1)\n",
       "  )\n",
       "  (indexer): LinearRegressionModel(\n",
       "    (linear): Linear(in_features=15, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitIndexer(mapper, indexer)\n",
    "model.load_state_dict(torch.load(\"models/baseembed_lego.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "for strs, idxs in test_dataloader:\n",
    "    pred_idx = round(model(strs).item())\n",
    "    print(pred_idx, idxs.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
